# TODO: Need to tidy all this up!

if __name__ == "__main__":

    def sanity_check(sample):
        print("Sanity Check - Start")
        print(f'sample length: {len(sample["input_ids"])} tokens')
        print(f'hidden shapes: {sample["additional_hidden_states"].shape}')

        model_path = "/proj/mtklmadm/models/pythia_all/" + model_name
        model = GPTNeoXForCausalLM.from_pretrained(model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        output1 = model(sample["input_ids"].unsqueeze(0))
        # print(output1.logits.shape)
        output2 = model.gpt_neox.layers[-1](
            sample["additional_hidden_states"].unsqueeze(0)
        )
        output2 = model.gpt_neox.final_layer_norm(output2[0])
        output2 = model.embed_out(output2)
        # print(output2.shape)
        assert (output1.logits == output2).any()
        print("Sanity Check - Pass")
        print("Output logits equal to logits computed from loaded hidden states")

    model_name = "pythia-160m"
    n_sample = 100
    chosen_layer = 1  # 2 means the last layer, 0 means the N-2th layer
    pile_files = [
        "05"
    ]  # file id ranging from 5-15, please check /nobackup/gpu_d_09023_t2/stutter
    train_dataset = CustomPileDataset(
        model_name, n_sample, chosen_layer, pile_files=["05"]
    )
    print(
        f"Succesfully loaded {len(train_dataset)} samples and their hiddens generated by {model_name}"
    )

    sanity_check(train_dataset[0])
