
optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 0.001
    betas: [0.9, 0.95]
    eps: 1e-8
    weight_decay: 0.1

# lr_scheduler:
#   class_path: torch.optim.lr_scheduler.CosineAnnealingLR
#   init_args:
#     T_max: 143000
#     eta_min: 0.0001
#   interval: step
#   frequency: 1
  # monitor: val_loss
  # warmup_steps: 1430